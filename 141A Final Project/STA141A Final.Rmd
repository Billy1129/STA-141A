---
title: "Final project"
author: "Yuzhen Zhang 918177232"
date: "2023-05-26"
output: html_document
---

## Abstract:

In this project, we analyze a subset of data collected by Steinmetz et al. (2019) from experiments conducted on 10 mice over 39 sessions. We focus on 18 sessions from four specific mice: Cori, Forssman, Hence, and Lederberg.Our analysis begins by exploring the data structures across the 18 sessions using data frames. We then examine the neural activities and changes during each trial and assess the homogeneity and heterogeneity across sessions and mice.To integrate the data, we extract shared patterns across sessions using a benchmark method. Finally, we build a predictive model using logistic regression (`glm()` function) based on the integrated data. This project aims to uncover insights into the relationship between neural activity and decision-making processes.

## Introduction:

This project aims to develop a predictive model that uses neural activity data and visual stimuli to accurately predict the outcome of trials. The dataset used is a subset of recordings obtained from experiments conducted by Steinmetz et al. (2019) on 10 mice over 39 sessions. Our analysis will focus on 18 sessions from four mice: Cori, Frossman, Hence, and Lederberg.

Each trial in the experiment involved presenting visual stimuli with varying contrast levels on two screens positioned beside the mice. The contrast levels could be 0, 0.25, 0.5, or 1, with 0 indicating no stimulus. The mice made decisions based on these stimuli using a wheel controlled by their forepaws. Feedback, categorized as success (1) or failure (-1), was given to the mice based on their wheel-turning behavior.

The neural activity of the mice's visual cortex was recorded in the form of spike trains. We will focus on the spike trains from stimulus onset to 0.4 seconds post-onset. The key variables in the dataset include: `feedback_type`: Type of feedback given to the mice, with 1 representing success and -1 representing failure. `contrast_left`: Contrast level of the left stimulus. `contrast_right`: Contrast level of the right stimulus. `time`: Centers of time bins for spike train data. `spks`: Number of spikes recorded from neurons in the visual cortex in each time bin. `brain_area`: The area of the brain where each neuron is located.

In this project, we will address three main parts: exploratory data analysis, data integration, and model training and prediction. In Part 1, we will conduct exploratory data analysis to describe the data structures across sessions, examine neural activities during trials, explore changes across trials, and assess homogeneity and heterogeneity across sessions and mice. This analysis will provide insights into the characteristics and patterns of the data, aiding us in building an effective prediction model.

Part 2 will focus on data integration, where we will propose methods to combine data across trials. This may involve identifying shared patterns across sessions or accounting for differences between sessions. The goal is to leverage the available information from multiple sessions, enabling us to enhance the prediction performance of our model.

Finally, in Part 3, we will develop a predictive model to accurately predict the outcome (feedback types) of the trials. The model's performance will be evaluated using two test sets consisting of 100 randomly selected trials from Session 1 and Session 18, respectively. The results from this analysis will have real-world implications as they can contribute to our understanding of the relationship between neural activity and decision-making processes. Additionally, the predictive model developed in this project may find applications in various domains, such as neuroscience research and clinical studies.

```{r}
suppressWarnings(library(tidyverse))
suppressWarnings(library(knitr))
suppressWarnings(library(dplyr))
library(kableExtra)

```

```{r}
session=list()
for(i in 1:18){
  session[[i]]=readRDS(paste('./sessions/session',i,'.rds',sep=''))
}

## Read test data
test_session = list()
for (i in 1:2) {
  test_session[[i]]=readRDS(paste('./test/test',i,'.rds',sep=''))
  
}
```

Part 1(i): The package contains 18 different sessions involving four different mice and each sessions contain different amount of trails. Our project includes 6 variables and they are `mouse_name` (the name of the mouse for specific sessions), `date_exp` (the date of the experiment), `num_brain_area`(the unique brain area involved). `num_neurons`(number of neurons) , `num_trails`(number of trails in each session), and `success_rate` (the ratio of successful trails to the total number of trails) . Note: our selected data does not contain any missing values.

```{r}
# Create data frame across sessions
n.session=length(session) 
meta <- tibble(
  mouse_name = rep('name',n.session),
  date_exp =rep('dt',n.session),
  num_brain_area = rep(0,n.session),
  num_neurons = rep(0,n.session),
  num_trials = rep(0,n.session),
  success_rate = rep(0,n.session)
)
# Store data into the data frame
for(i in 1:n.session){
  tmp = session[[i]];
  meta[i,1]=tmp$mouse_name;
  meta[i,2]=tmp$date_exp;
  meta[i,3]=length(unique(tmp$brain_area));
  meta[i,4]=dim(tmp$spks[[1]])[1];
  meta[i,5]=length(tmp$feedback_type);
  meta[i,6]=mean(tmp$feedback_type+1)/2;
}

table_footnote <- "Table 1.Data Structure across Sessions"

meta_table <- meta %>%
  kable() %>%
  add_header_above(c('Selected Data with Six Variables' = 6)) %>%
  kable_styling() %>%
  add_footnote(c(table_footnote))

meta_table
```

```{r}
i.s=18 # indicator for this session

i.t=8 # indicator for this trial 

spk.trial = session[[i.s]]$spks[[i.t]]
area=session[[i.s]]$brain_area

spk.count=apply(spk.trial,1,sum)

spk.average.tapply=tapply(spk.count, area, mean)


# dplyr: 
# To use dplyr you need to create a data frame
tmp <- data.frame(
  area = area,
  spikes = spk.count
)
# Calculate the average by group using dplyr
spk.average.dplyr =tmp %>%
  group_by(area) %>%
  summarize(mean= mean(spikes))
```

```{r}
average_spike_area<-function(i.t,this_session){
  spk.trial = this_session$spks[[i.t]]
  area= this_session$brain_area
  spk.count=apply(spk.trial,1,sum)
  spk.average.tapply=tapply(spk.count, area, mean)
  return(spk.average.tapply)
  }

# Test the function
average_spike_area(1,this_session = session[[i.s]])
```

```{r}
n.trial=length(session[[i.s]]$feedback_type)
n.area=length(unique(session[[i.s]]$brain_area ))
# Alternatively, you can extract these information in the meta that we created before.

# We will create a data frame that contain the average spike counts for each area, feedback type,  the two contrasts, and the trial id

trial.summary =matrix(nrow=n.trial,ncol= n.area+1+2+1)
for(i.t in 1:n.trial){
  trial.summary[i.t,]=c(average_spike_area(i.t,this_session = session[[i.s]]),
                          session[[i.s]]$feedback_type[i.t],
                        session[[i.s]]$contrast_left[i.t],
                        session[[i.s]]$contrast_right[i.s],
                        i.t)
}

colnames(trial.summary)=c(names(average_spike_area(i.t,this_session = session[[i.s]])), 'feedback', 'left contr.','right contr.','id' )

# Turning it into a data frame
trial.summary <- as_tibble(trial.summary)
```

Part 1(iii): In this case I used Session 18 as an example to explore the neural activities during each trail. From the plot we can clearly see that `root` neural activity has highest average spikes counts in all trails and `CP` activity has lowest average spikes counts.

```{r}
area.col=rainbow(n=n.area,alpha=0.7)
# In base R, I usually initiate a blank plot before drawing anything on it
plot(x=1,y=0, col='white',xlim=c(0,n.trial),ylim=c(0.5,3), xlab="Trials",ylab="Average spike counts", main=paste("Spikes per area in Session", i.s))


for(i in 1:n.area){
  lines(y=trial.summary[[i]],x=trial.summary$id,col=area.col[i],lty=2,lwd=1)
  lines(smooth.spline(trial.summary$id, trial.summary[[i]]),col=area.col[i],lwd=3)
  }
legend("topright", 
  legend = colnames(trial.summary)[1:n.area], 
  col = area.col, 
  lty = 1, 
  cex = 0.8
)
mtext("Figure 1. Average spike count across trails in session 18", side = 1, line = 4, font = 2)

```

```{r}
plot.trial<-function(i.t,area, area.col,this_session){
    
    spks=this_session$spks[[i.t]];
    n.neuron=dim(spks)[1]
    time.points=this_session$time[[i.t]]
    
    plot(0,0,xlim=c(min(time.points),max(time.points)),ylim=c(0,n.neuron+1),col='white', xlab='Time (s)',yaxt='n', ylab='Neuron', main=paste('Trial ',i.t, 'feedback', this_session$feedback_type[i.t] ),cex.lab=1.5)
    for(i in 1:n.neuron){
        i.a=which(area== this_session$brain_area[i]);
        col.this=area.col[i.a]
        
        ids.spike=which(spks[i,]>0) # find out when there are spikes 
        if( length(ids.spike)>0 ){
            points(x=time.points[ids.spike],y=rep(i, length(ids.spike) ),pch='.',cex=4, col=col.this)
        }
      
            
    }
    
legend("topright", 
  legend = area, 
  col = area.col, 
  pch = 16, 
  cex = 0.8
  )
}

varname=names(trial.summary);
area=varname[1:(length(varname)-4)]
```

Part 1(ii): I choose Session 18 trail 1,5,10,15,20 to explore the neural activities during each trail. After putting different color to different neural activities we can clearly see that there are minor changing throughout 5 trails. Overall `root` neural activity has highest neuron across trails and `CP` has lowest neuron.

```{r}
varname=names(trial.summary);
area=varname[1:(length(varname)-4)]
par(mfrow=c(1,1))
plot.trial(1,area, area.col,session[[i.s]])
mtext("Figure 2.1 Session 18 Trail 1 feedback", side = 1, line = 4.1, font = 1)
plot.trial(5,area, area.col,session[[i.s]])
mtext("Figure 2.2 Session 18 Trail 5 feedback", side = 1, line = 4.1, font = 1)

plot.trial(10,area, area.col,session[[i.s]])
mtext("Figure 2.3 Session 18 Trail 10 feedback", side = 1, line = 4.1, font = 1)

plot.trial(15,area, area.col,session[[i.s]])
mtext("Figure 2.4 Session 18 Trail 15 feedback", side = 1, line = 4.1, font = 1)

plot.trial(20,area, area.col,session[[i.s]])
mtext("Figure 2.5 Session 18 Trail 20 feedback", side = 1, line = 4.1, font = 1)


```

Part 1(iv): The graph below is the neuron count per brain area vs. success rate for 4 different mice. From the data selected from 18 sessions we know that Cori has 3 sessions, Forssmann has 4 sessions, Hench has 4 sessions and Lederberg has 7 sessions. each point represents different session we observed. From the graph we can see Lederberg has highest success rate overall and other three mice has lower rate but the success rate of other three mice are getting larger as number of neuron per brain area increases.

```{r}
meta$mouse_name <- as.factor(meta$mouse_name)

meta$num_new_per_area <- meta$num_neurons / meta$num_brain_area

ggplot(meta, aes(x = num_new_per_area, y = success_rate, group = mouse_name, color = mouse_name)) +
  geom_line() +
  geom_point() +
  labs(x = "Number of Neurons per Area", y = "Success Rate", 
       title = "Number of Neurons per Area vs. Success Rate", 
       caption = "Figure 3. Number of Neurons per Area vs. Success Rate across mice") +
  theme_minimal()

```

Part 2: For the integration part I choose Benchmark method to integrate data. For each trail, we firstly take the summation of spikes for each neuron, which results in a vector that contains the total number of spikes for all neurons during the 0.4 seconds in that trail; then, we take the average of the total number of spikes, which results in one number that is the average spike counts during that trail. Ultimately I will build my predictive model based on this integrated data.

```{r}
# Create an empty dataframe to store the data
df <- data.frame()  

# Loop over the sessions
for (i in 1:length(session)) {
  tmp <- session[[i]]
  n_trials <- length(tmp$contrast_left)  
  
  # Loop over the trials within each session
  for (j in 1:n_trials) {
    spks_trial <- tmp$spks[[j]]
    total_spikes <- apply(spks_trial, 1, sum)
    ave_spikes <- mean(total_spikes)
    
    left_contract <- tmp$contrast_left[j]
    right_contract <- tmp$contrast_right[j]
    num_neu <- dim(tmp$spks[[j]])[1]
    
    feedback_type <- tmp$feedback_type[j] 
    
    df <- rbind(df, data.frame(
      n_sessions = i,
      n_trials = j,
      AveSpkPerNeu = ave_spikes,
      leftContract = left_contract,
      rightContract = right_contract,
      NumNeu = num_neu,
      feedbackType = feedback_type
    ))
  }
}
 
# Replace all -1 values to 0
df$feedbackType <- ifelse(df$feedbackType == -1, 0,1)

head(df)
```

```{r}
## Test data:
test_df <- data.frame()
for (i in 1:length(test_session)) {
  test_tmp <- test_session[[i]]
  n_trials <- length(test_tmp$contrast_left)  
  
  # Loop over the trials within each session
  for (j in 1:n_trials) {
    spks_trial <- test_tmp$spks[[j]]
    total_spikes <- apply(spks_trial, 1, sum)
    ave_spikes <- mean(total_spikes)
    
    left_contract <- test_tmp$contrast_left[j]
    right_contract <- test_tmp$contrast_right[j]
    num_neu <- dim(test_tmp$spks[[j]])[1]
    
    feedback_type <- test_tmp$feedback_type[j] 
    
    test_df <- rbind(test_df, data.frame(
      n_sessions = i,
      n_trials = j,
      AveSpkPerNeu = ave_spikes,
      leftContract = left_contract,
      rightContract = right_contract,
      NumNeu = num_neu,
      feedbackType = feedback_type
    ))
  }
}
 
# Replace all -1 values to 0
test_df$feedbackType <- ifelse(test_df$feedbackType == -1, 0,1)

head(test_df)
```

Part 3. we build our predictive model based on the integrated data. We employ a Logistic Regression model using the `glm()` function. This model allows us to predict the outcome, specifically the feedback type, of each trial based on the neural activity data and stimuli conditions.

```{r}
suppressWarnings(library(caret))

set.seed(155) # Set a seed for reproducibility

train_data <- df[df$n_sessions == 5 | df$n_sessions == 16, ] 
test_data <- test_df

# Fit the logistic regression model
model <- glm(feedbackType ~ AveSpkPerNeu + leftContract + rightContract , data = train_data, family = "binomial")

# Predict on the test data
test_predictions <- predict(model, newdata = test_data, type = "response")
test_predictions <- ifelse(test_predictions > 0.5, 1, 0)

# Compare predicted values with actual values
correct_predictions <- test_predictions == test_data$feedbackType

# Calculate accuracy
accuracy <- sum(correct_predictions) / length(correct_predictions)

# Create the confusion matrix
confusion_matrix <- caret::confusionMatrix(data = factor(test_predictions), reference = factor(test_data$feedbackType))
confusion_matrix
F1_Score <- confusion_matrix$byClass["F1"]
Recall <- confusion_matrix$byClass["Sensitivity"]
Precision <- confusion_matrix$byClass["Pos Pred Value"]
Missclassification_Rate <- confusion_matrix$overall["Accuracy"]

cat("F1-Score:", F1_Score, "\n")
cat("Recall:", Recall, "\n")
cat("Precision:", Precision, "\n")
cat("Missclassification Rate:", 1 - Missclassification_Rate, "\n")
```

## Discussion

The logistic regression model trained on the given data achieved an accuracy of 0.745, indicating that it correctly classified approximately 74.5% of the instances. The model's performance was evaluated using a confusion matrix, which showed that out of the actual negative instances, 6 were correctly classified as negative, while 2 were incorrectly classified as positive. Similarly, out of the actual positive instances, 143 were correctly classified as positive, while 49 were incorrectly classified as negative. The sensitivity (also known as recall) of the model was 0.109, indicating that it had a low ability to correctly identify positive instances. The specificity was high at 0.986, indicating a strong ability to correctly identify negative instances. The positive predictive value (precision) was 0.75, suggesting that when the model predicted a positive outcome, it was correct 75% of the time. The F1-score, which combines precision and recall, was 0.190. Overall, the model's performance is moderate, with room for improvement, especially in terms of correctly identifying positive instances.

## Session Info

```{r}
sessionInfo()
```

## References

1.  ChatGpt

2.  Stack Overflow

3.  Discussion Note

4.  Lecture Note
